---
layout: default
title: "Research"
---
<div class="content">
	<h2>Research</h2>
	
	<p>I am a member of the <b>University of Virginia Dependable Systems and Analytics Group</b> 
		(<a href="https://homa-alem.github.io/research.html"><b>UVA-DSA</b></a>). Our research focuses on the design and 
		validation of Resilient Cyber-Physical Systems (CPS) with applications in surgical robotics, medical devices,
		and autonomous systems. </p>
		
	<p>My research focuses on improving the safety of robot-assisted surgery (RAS) by developing generalizable, objective, 
		and transparent methods for the analysis of surgical data. I am developing a formal framework for generalized 
		and hierarchical modeling of surgical activities and have created the COntext and Motion Primitive Aggregate 
		Surgical Set (<a href="https://github.com/UVA-DSA/COMPASS">COMPASS</a>). This dataset is used for fine-grained 
		activity recognition and runtime context inference, and can support surgeons during training and real operations 
		with error detection, skill assessment, and autonomy.</p>
	
	<h3>Projects</h3>
	
	<br>
	
	<tr>
		<td><b> COMPASS: a formal framework and aggregate dataset for generalized surgical procedure modeling </b></td>
	</tr>
	<p>This project proposed a formal framework for the modeling and segmentation of minimally invasive surgical tassks using
		a unified set of motion primitives (MPs) to enable more objective labeling and the aggregation of different
		datasets. We create the COntext and Motion Primitive Aggregate Surgical Set (COMPASS) including six dry-lab
		surgical tasks from three publicly available datasets (JIGSAWS, DESK, and ROSMA) with kinematic and video data 
		and context and MP labels. </p>
	<p>Dataset available on <a href="https://github.com/UVA-DSA/COMPASS">GitHub</a></p>
	
	<tr>
		<td><center><img src="/images/NP_fig.png" width = "800" height="auto"></center><br></td>
	<td></td>
	</tr>
	
	
	<br><br>
	
	
	<tr>
		<td><b> MIDAS: Multi-modal Image and Data Acquisition System </b></td>
	</tr>
	
	<p>This project is developing a collection system for the simultaneous and synchronized acquisition of data from 
		multiple sensors and modalities: master console video via video capture, master console manipulator 
		kinematics via magnetic tracking, wrist accelerometers via smart watches, and RAVEN II surgical robot
		kinematics.</p>
	
	<br><br>
	
	<tr>
		<td><b>Analysis of executional and procedural errors in dry-lab robotic surgery experiments</b></td>
	</tr>

	<p>Code and error labels available on <a href="https://github.com/UVA-DSA/ExecProc_Error_Analysis">GitHub</a></p>
	<p>This project created a rubric for identifying task and gesture-specific Executional and Procedural errors and
	evaluated dry-lab demonstrations of the Suturing and Needle Passing tasks in the JIGSAWS dataset. We labeled video
	data for erroneous gestures and analyzed kinematic data to identify parameters that distinguish errors.</p>
	
	<br><br>
	
	<tr>
		<td><b>Reactive autonomous camera system </b></td>
	</tr>
	
	<p>This project created a robotic arm for camera manipulation that worked independently from our lab's RAVEN II surgical 
		robot. In manual control mode, it accepts commands from foot pedals. In autonomous control mode, it uses an MRCNN
		to localize and classify objects of interest within the view of the stereoscopic camera and applies a set of
		control rules to keep those objects in view.</p>
	<p>Code availabe on <a href="https://github.com/kch4fk/IBIS">GitHub</a></p>
	<tr>
		<td><center><img src="/images/IBIS.png" width="450" height="auto"></center><br></td>
	<td></td>
	</tr>
	
	<br><br>
	
	<h4>Acknowledgements</h4>
	<p>This research is supported by funding from the National Scicence Foundation. </p>
	
	<td><img src="/images/nsf_logo.jpg" width="80" height="auto" ></td>
	
	
	<!--<p>Text <em>emphasis</em> Text <a href="/about">Read more</a></p>-->
	<!--<p>Check out what my <a href="https://homa-alem.github.io/research.html">lab</a> is working on.</p> -->
</div><!-- /.blurb -->
